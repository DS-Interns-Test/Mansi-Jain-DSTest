{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:50.091937Z",
     "iopub.status.busy": "2021-07-29T10:31:50.090929Z",
     "iopub.status.idle": "2021-07-29T10:31:50.883120Z",
     "shell.execute_reply": "2021-07-29T10:31:50.881831Z",
     "shell.execute_reply.started": "2021-07-29T10:31:50.091789Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:50.885296Z",
     "iopub.status.busy": "2021-07-29T10:31:50.884770Z",
     "iopub.status.idle": "2021-07-29T10:31:53.261812Z",
     "shell.execute_reply": "2021-07-29T10:31:53.260941Z",
     "shell.execute_reply.started": "2021-07-29T10:31:50.885245Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from random import *\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU, Activation, BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:53.263607Z",
     "iopub.status.busy": "2021-07-29T10:31:53.263164Z",
     "iopub.status.idle": "2021-07-29T10:31:53.272792Z",
     "shell.execute_reply": "2021-07-29T10:31:53.271525Z",
     "shell.execute_reply.started": "2021-07-29T10:31:53.263574Z"
    }
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "from subprocess import check_output\n",
    "with open('../input/iam-handwriting-top50/forms_for_parsing.txt') as f:\n",
    "    for line in f:\n",
    "        key = line.split(' ')[0]\n",
    "        writer = line.split(' ')[1]\n",
    "        d[key] = writer\n",
    "print(len(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:53.274815Z",
     "iopub.status.busy": "2021-07-29T10:31:53.274438Z",
     "iopub.status.idle": "2021-07-29T10:31:54.129732Z",
     "shell.execute_reply": "2021-07-29T10:31:54.128535Z",
     "shell.execute_reply.started": "2021-07-29T10:31:53.274766Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = []\n",
    "target_list = []\n",
    "\n",
    "path_to_files = os.path.join('../input/iam-handwriting-top50/data_subset/data_subset', '*')\n",
    "for filename in sorted(glob.glob(path_to_files)):\n",
    "#     print(filename)\n",
    "    tmp.append(filename)\n",
    "    image_name = filename.split('/')[-1]\n",
    "    file, ext = os.path.splitext(image_name)\n",
    "    parts = file.split('-')\n",
    "    form = parts[0] + '-' + parts[1]\n",
    "    for key in d:\n",
    "        if key == form:\n",
    "            target_list.append(str(d[form]))\n",
    "\n",
    "img_files = np.asarray(tmp)\n",
    "img_targets = np.asarray(target_list)\n",
    "print(img_files.shape)\n",
    "print(img_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:54.131776Z",
     "iopub.status.busy": "2021-07-29T10:31:54.131299Z",
     "iopub.status.idle": "2021-07-29T10:31:54.723287Z",
     "shell.execute_reply": "2021-07-29T10:31:54.722223Z",
     "shell.execute_reply.started": "2021-07-29T10:31:54.131727Z"
    }
   },
   "outputs": [],
   "source": [
    "for filename in img_files[:3]:\n",
    "    img=mpimg.imread(filename)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(img, cmap ='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:54.724960Z",
     "iopub.status.busy": "2021-07-29T10:31:54.724628Z",
     "iopub.status.idle": "2021-07-29T10:31:54.733835Z",
     "shell.execute_reply": "2021-07-29T10:31:54.732589Z",
     "shell.execute_reply.started": "2021-07-29T10:31:54.724928Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(img_targets)\n",
    "encoded_Y = encoder.transform(img_targets)\n",
    "\n",
    "print(img_files[:5], img_targets[:5], encoded_Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:54.736095Z",
     "iopub.status.busy": "2021-07-29T10:31:54.735528Z",
     "iopub.status.idle": "2021-07-29T10:31:54.752301Z",
     "shell.execute_reply": "2021-07-29T10:31:54.751038Z",
     "shell.execute_reply.started": "2021-07-29T10:31:54.736057Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files, rem_files, train_targets, rem_targets = train_test_split(\n",
    "        img_files, encoded_Y, train_size=0.66, random_state=52, shuffle= True)\n",
    "\n",
    "validation_files, test_files, validation_targets, test_targets = train_test_split(\n",
    "        rem_files, rem_targets, train_size=0.5, random_state=22, shuffle=True)\n",
    "\n",
    "print(train_files.shape, validation_files.shape, test_files.shape)\n",
    "print(train_targets.shape, validation_targets.shape, test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:54.755034Z",
     "iopub.status.busy": "2021-07-29T10:31:54.754689Z",
     "iopub.status.idle": "2021-07-29T10:31:54.771640Z",
     "shell.execute_reply": "2021-07-29T10:31:54.770386Z",
     "shell.execute_reply.started": "2021-07-29T10:31:54.754996Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8 \n",
    "num_classes = 50\n",
    "\n",
    "def generate_data(samples, target_files,  batch_size=batch_size, factor = 0.1 ):\n",
    "    num_samples = len(samples)\n",
    "    from sklearn.utils import shuffle\n",
    "    while 1: \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            batch_targets = target_files[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            targets = []\n",
    "            for i in range(len(batch_samples)):\n",
    "                batch_sample = batch_samples[i]\n",
    "                batch_target = batch_targets[i]\n",
    "                im = Image.open(batch_sample)\n",
    "                cur_width = im.size[0]\n",
    "                cur_height = im.size[1]\n",
    "\n",
    "                height_fac = 113 / cur_height\n",
    "\n",
    "                new_width = int(cur_width * height_fac)\n",
    "                size = new_width, 113\n",
    "\n",
    "                imresize = im.resize((size), Image.ANTIALIAS)  \n",
    "                now_width = imresize.size[0]\n",
    "                now_height = imresize.size[1]\n",
    "\n",
    "                avail_x_points = list(range(0, now_width - 113 ))# total x start points are from 0 to width -113\n",
    "\n",
    "                pick_num = int(len(avail_x_points)*factor)\n",
    "\n",
    "                random_startx = sample(avail_x_points,  pick_num)\n",
    "\n",
    "                for start in random_startx:\n",
    "                    imcrop = imresize.crop((start, 0, start+113, 113))\n",
    "                    images.append(np.asarray(imcrop))\n",
    "                    targets.append(batch_target)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(targets)\n",
    "\n",
    "            X_train = X_train.reshape(X_train.shape[0], 113, 113, 1)\n",
    "            #convert to float and normalize\n",
    "            X_train = X_train.astype('float32')\n",
    "          X_train /= 255\n",
    "\n",
    "            #One hot encode y\n",
    "            y_train = to_categorical(y_train, num_classes)\n",
    "            yield shuffle(X_train), shuffle(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:54.773712Z",
     "iopub.status.busy": "2021-07-29T10:31:54.773198Z",
     "iopub.status.idle": "2021-07-29T10:31:54.788420Z",
     "shell.execute_reply": "2021-07-29T10:31:54.787575Z",
     "shell.execute_reply.started": "2021-07-29T10:31:54.773666Z"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = generate_data(train_files, train_targets, batch_size=batch_size, factor = 0.3)\n",
    "validation_generator = generate_data(validation_files, validation_targets, batch_size=batch_size, factor = 0.3)\n",
    "test_generator = generate_data(test_files, test_targets, batch_size=batch_size, factor = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:54.790386Z",
     "iopub.status.busy": "2021-07-29T10:31:54.789864Z",
     "iopub.status.idle": "2021-07-29T10:31:55.148666Z",
     "shell.execute_reply": "2021-07-29T10:31:55.147551Z",
     "shell.execute_reply.started": "2021-07-29T10:31:54.790319Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_image(image):\n",
    "    import tensorflow as tf\n",
    "    return tf.image.resize(image,[56,56])\n",
    "\n",
    "row, col, ch = 113, 113, 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(row, col, ch)))\n",
    "\n",
    "# Resise data within the neural network\n",
    "model.add(Lambda(resize_image))  #resize images to allow for easy computation\n",
    "\n",
    "# CNN model - Building the model suggested in research paper\n",
    "\n",
    "model.add(Convolution2D(filters= 32, kernel_size =(5,5), strides= (2,2), padding='same', name='conv1')) #96\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool1'))\n",
    "\n",
    "model.add(Convolution2D(filters= 64, kernel_size =(3,3), strides= (1,1), padding='same', name='conv2'))  #256\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool2'))\n",
    "\n",
    "model.add(Convolution2D(filters= 128, kernel_size =(3,3), strides= (1,1), padding='same', name='conv3'))  #256\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2), name='pool3'))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, name='dense1'))  \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, name='dense2'))  \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes,name='output'))\n",
    "model.add(Activation('softmax'))  #softmax since output is within 50 classes\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:55.150414Z",
     "iopub.status.busy": "2021-07-29T10:31:55.150054Z",
     "iopub.status.idle": "2021-07-29T10:31:55.167329Z",
     "shell.execute_reply": "2021-07-29T10:31:55.166046Z",
     "shell.execute_reply.started": "2021-07-29T10:31:55.150369Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-29T10:31:55.169842Z",
     "iopub.status.busy": "2021-07-29T10:31:55.169265Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_epoch = 8\n",
    "\n",
    "samples_per_epoch = 3268\n",
    "nb_val_samples = 842\n",
    "\n",
    "#save every model using Keras checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights.{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, verbose=1, save_best_only=False)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#Model fit generator\n",
    "history_object = model.fit(train_generator, steps_per_epoch = samples_per_epoch//batch_size,\n",
    "                                      validation_data=validation_generator,\n",
    "                                      validation_steps=nb_val_samples, epochs=nb_epoch, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "scores = model.evaluate_generator(test_generator,842) \n",
    "print(\"Accuracy = \", scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
